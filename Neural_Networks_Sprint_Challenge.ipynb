{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W9D4 - JHuneycutt - Neural Networks Sprint Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "aD8Yb0P8D04e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Networks Coding Challenge\n",
        "\n",
        "Objectives:\n",
        "  * Write a simple three layer network\n",
        "  * Compute forward propagation for a new sample in three layer network\n",
        "  * Compute backward propagation in the same network\n",
        "  * Use MLPClassifier to train and test a dataset\n",
        "\n",
        "### Background\n",
        "\n",
        "Other than the MLPClassifier objective, you will be working with this neural net during this coding challenge:\n",
        "\n",
        "![Simple Neural Net](https://www.lucidchart.com/publicSegments/view/a5b0773e-7165-450d-99fc-7089891e099a/image.png)"
      ]
    },
    {
      "metadata": {
        "id": "9JrCk2HEPwoI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Write a simple three layer network\n",
        "\n",
        "Create variables to store weights and biases for the above network. Initialize each with $0.5$."
      ]
    },
    {
      "metadata": {
        "id": "5UeD3N5_PwCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQGEW9na8mDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create weights\n",
        "# np.random.seed(42)\n",
        "W1,W2,W3 = 0.5, 0.5, 0.5\n",
        "\n",
        "# create biases\n",
        "# np.random.seed(42)\n",
        "b1,b2,b3 = 0.5, 0.5, 0.5\n",
        "\n",
        "# sigmoid function\n",
        "def sigma(x):\n",
        "    return 1 / (1+np.exp(-x))\n",
        "  \n",
        "# def NN(x1):\n",
        "#     h1 = sigma(x1*w1 + b1*1)\n",
        "#     h2 = sigma(x1*w2 + b2*1)\n",
        "#     z = sigma(h1*w3 + h2*w4 + b3*1)\n",
        "#     return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSfpHHfJFPc5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Compute forward propagation for a new sample in three layer network\n",
        "\n",
        "Write a function `feed_forward` that takes a new sample $x$ and calculates $\\hat{y}$ via forward propagation."
      ]
    },
    {
      "metadata": {
        "id": "oI260RxPrUV7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x1 = 4\n",
        "x2 = 0.5\n",
        "x3 = 0.125\n",
        "y1 = 0\n",
        "y2 = 1\n",
        "y3 = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x3K306_pFUQc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b1f152f7-cbf7-4b47-8dc4-b02a5ce70d2b"
      },
      "cell_type": "code",
      "source": [
        "# This is the forward propagation function\n",
        "def feed_forward(a0):\n",
        "\n",
        "    # Do the first Linear step \n",
        "    z1 = np.dot(a0,W1) + b1\n",
        "    \n",
        "    # Put it through the first activation function\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # Second linear step\n",
        "    z2 = np.dot(a1,W2) + b2\n",
        "    \n",
        "    # Put through second activation function\n",
        "    a2 = np.tanh(z2)\n",
        "    \n",
        "    #Third linear step\n",
        "    z3 = np.dot(a2, W3) + b3\n",
        "    \n",
        "    #For the Third linear activation function we use the sigmoid function\n",
        "    a3 = sigma(z3)\n",
        "    \n",
        "    #Store all results in these values\n",
        "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
        "    yhat = a3\n",
        "    return yhat, a0, a1, a2, a3\n",
        "    \n",
        "  \n",
        "# TEST\n",
        "y_hat1 = feed_forward(x1)\n",
        "y_hat2 = feed_forward(x2)\n",
        "y_hat3 = feed_forward(x3)\n",
        "\n",
        "print(\"===y_hat1===\")\n",
        "print(y_hat1[0])\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===y_hat2===\")\n",
        "print(y_hat2[0])\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"===y_hat3===\")\n",
        "print(y_hat3[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===y_hat1===\n",
            "0.7066946552192437\n",
            "\n",
            "\n",
            "===y_hat2===\n",
            "0.6978063882895045\n",
            "\n",
            "\n",
            "===y_hat3===\n",
            "0.6940316729708891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xS6VlmpYTzPx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Compute backward propagation for the same network\n",
        "\n",
        "The backprop algorithm is derived from the goal of minimizing the error (or loss) function $\\epsilon = (y - \\hat{y})^2$.\n",
        "\n",
        "$\\epsilon = (y - \\sigma(h_2+b_2))^2$\n",
        "\n",
        "Via the chain rule, the derivative of the above is\n",
        "\n",
        "$\\frac{\\partial \\epsilon}{\\partial \\hat{y}} = 2(y-\\hat{y})\\sigma(h_2)$\n",
        "\n",
        "Let $\\alpha = 0.1$. Update the weights for $h_2$ and $h_1$ via back propagation so that $h_2$ = $h_2 + \\alpha \\frac{\\partial \\epsilon}{\\partial \\hat{y}}$ and $h_1 = h_1 + \\alpha \\frac{\\partial \\epsilon}{\\partial h_2}$\n",
        "\n",
        "Also, let $\\sigma(x) = ReLU(x)$. As such, $\\sigma'(x) = 0$ when $x \\le 0$ and $\\sigma'(x) = 1$ when $x \\gt 0$.\n",
        "\n",
        "Check Case1: of [Brian Dolhansky](http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4) for a more detailed explanation of the values in the back propagation.\n"
      ]
    },
    {
      "metadata": {
        "id": "Fv_8vZvrT44v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a040297b-1d8f-4db1-f73b-3f87d0a64bb4"
      },
      "cell_type": "code",
      "source": [
        "# This is the backward propagation function\n",
        "def feed_forward_and_back_propagate(x,y):\n",
        "\n",
        "    # derivative of tanh\n",
        "    def dtanh(y):\n",
        "        return 1 - y*y\n",
        "  \n",
        "  \n",
        "    # Load forward propagation results\n",
        "    a0 = feed_forward(x)[1]\n",
        "    a1 = feed_forward(x)[2]\n",
        "    a2 = feed_forward(x)[3]\n",
        "    a3 = feed_forward(x)[4]\n",
        "    \n",
        "    \n",
        "    # Get number of samples\n",
        "    m = 1\n",
        "    \n",
        "    # Calculate loss derivative with respect to output\n",
        "    dz3 = a3 - y\n",
        "\n",
        "    # Calculate loss derivative with respect to second layer weights\n",
        "    dW3 = 1/m*np.dot(a2.T,dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
        "    \n",
        "    # Calculate loss derivative with respect to second layer bias\n",
        "    db3 = 1/m*np.sum(dz3, axis=0)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer\n",
        "    dz2 = np.multiply(np.dot(dz3, W3) ,dtanh(a2))\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer weights\n",
        "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer bias\n",
        "    db2 = 1/m*np.sum(dz2, axis=0)\n",
        "    \n",
        "    dz1 = np.multiply(np.dot(dz2, W2),dtanh(a1))\n",
        "    \n",
        "    dW1 = 1/m*np.dot(a0,dz1)\n",
        "    \n",
        "    db1 = 1/m*np.sum(dz1,axis=0)\n",
        "    \n",
        "    # forward again\n",
        "    \n",
        "    # Do the first Linear step \n",
        "    z1 = np.dot(x,dW1) + db1\n",
        "    \n",
        "    # Put it through the first activation function\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # Second linear step\n",
        "    z2 = np.dot(a1,dW2) + db2\n",
        "    \n",
        "    # Put through second activation function\n",
        "    a2 = np.tanh(z2)\n",
        "    \n",
        "    #Third linear step\n",
        "    z3 = np.dot(a2, dW3) + db3\n",
        "    \n",
        "    #For the Third linear activation function we use the sigmoid function\n",
        "    a3 = sigma(z3)\n",
        "    \n",
        "   \n",
        "    # Store gradients\n",
        "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
        "    return a3, grads\n",
        "\n",
        "\n",
        "\n",
        "# CODE\n",
        "y_hat4 = feed_forward_and_back_propagate(x1,y1)\n",
        "y_hat5 = feed_forward_and_back_propagate(x2,y2)\n",
        "y_hat6 = feed_forward_and_back_propagate(x3,y3)\n",
        "\n",
        "print(y_hat4)\n",
        "print(y_hat5)\n",
        "print(y_hat6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.6876401335843211, {'dW3': 0.5362179769516296, 'db3': 0.7066946552192437, 'dW2': 0.1479078346638543, 'db2': 0.14991454608045088, 'dW1': 0.007973123184948928, 'db1': 0.001993280796237232})\n",
            "(0.42904252993945663, {'dW3': -0.20360220447179556, 'db3': -0.3021936117104955, 'dW2': -0.052405227344898685, 'db2': -0.08250856298814166, 'dW1': -0.012305859435102911, 'db1': -0.024611718870205822})\n",
            "(0.42834303709452126, {'dW3': -0.19522982701336145, 'db3': -0.3059683270291109, 'dW2': -0.04624097647812385, 'db2': -0.0906988189402449, 'dW1': -0.0041952364448069, 'db1': -0.0335618915584552})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PYmuA8VwJ4_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Use MLPClassifier to train a dataset\n",
        "\n",
        "`X` is now a small dataset. Create an MLPClassifier from sklearn and train it on the `X` dataset, with `y` as the targets."
      ]
    },
    {
      "metadata": {
        "id": "uslUTP8rV0XS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.row_stack([x1,x2,x3])\n",
        "y = np.row_stack([y1,y2,y3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K1VobsFFV2_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ba8a71d0-ea2a-4d7c-d3fa-3816df5e3d0a"
      },
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.   ]\n",
            " [0.5  ]\n",
            " [0.125]]\n",
            "[[0]\n",
            " [1]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jcyi6Z16OGuM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "84aecdb7-5a2d-49e6-988c-606ba2059743"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# CODE\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "                    hidden_layer_sizes=(3, 3),\n",
        "                    activation='tanh',\n",
        "                    solver='sgd',\n",
        "                    alpha=1e-5,\n",
        "                    batch_size=100, \n",
        "                    learning_rate='adaptive',\n",
        "                    learning_rate_init=0.001,\n",
        "                    max_iter=200,\n",
        "                    shuffle=True,\n",
        "                    random_state=42,\n",
        "                    tol=1e-4 )\n",
        "\n",
        "mlp.fit(X,y)\n",
        "\n",
        "predictions = mlp.predict(X)\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(confusion_matrix(y,predictions))\n",
        "print(classification_report(y,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0]\n",
            " [0 2]]\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       1.00      1.00      1.00         1\n",
            "          1       1.00      1.00      1.00         2\n",
            "\n",
            "avg / total       1.00      1.00      1.00         3\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:912: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:358: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
            "  warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "eHNevw5tVJFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3573
        },
        "outputId": "24874ed9-572e-47e8-f4ae-f10bf22f3c00"
      },
      "cell_type": "code",
      "source": [
        "# neural net training with keras sequential\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# initializing the ANN\n",
        "classifier = Sequential()\n",
        "\n",
        "# add input layer and first hidden layer\n",
        "classifier.add(Dense(output_dim = 3, init = 'uniform', activation = 'relu', input_dim=X.shape[1]))\n",
        "\n",
        "# add 2nd hidden layer\n",
        "classifier.add(Dense(output_dim = 3, init = 'uniform', activation = 'relu'))\n",
        "\n",
        "# add output layer\n",
        "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "# compile ANN\n",
        "classifier.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# fit ANN to training set\n",
        "classifier.fit(X, y, batch_size=10, nb_epoch=100)\n",
        "\n",
        "# predictions\n",
        "y_pred = classifier.predict(X)\n",
        "\n",
        "# convert predictions into binary\n",
        "y_pred = (y_pred > 0.5) * 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=1, units=3, kernel_initializer=\"uniform\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=3, kernel_initializer=\"uniform\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 0.6931 - acc: 0.3333\n",
            "Epoch 2/100\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.6930 - acc: 0.6667\n",
            "Epoch 3/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6928 - acc: 0.6667\n",
            "Epoch 4/100\n",
            "3/3 [==============================] - 0s 989us/step - loss: 0.6926 - acc: 0.6667\n",
            "Epoch 5/100\n",
            "3/3 [==============================] - 0s 824us/step - loss: 0.6924 - acc: 0.6667\n",
            "Epoch 6/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6923 - acc: 0.6667\n",
            "Epoch 7/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6921 - acc: 0.6667\n",
            "Epoch 8/100\n",
            "3/3 [==============================] - 0s 863us/step - loss: 0.6919 - acc: 0.6667\n",
            "Epoch 9/100\n",
            "3/3 [==============================] - 0s 900us/step - loss: 0.6918 - acc: 0.6667\n",
            "Epoch 10/100\n",
            "3/3 [==============================] - 0s 895us/step - loss: 0.6916 - acc: 0.6667\n",
            "Epoch 11/100\n",
            "3/3 [==============================] - 0s 861us/step - loss: 0.6914 - acc: 0.6667\n",
            "Epoch 12/100\n",
            "3/3 [==============================] - 0s 949us/step - loss: 0.6912 - acc: 0.6667\n",
            "Epoch 13/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6911 - acc: 0.6667\n",
            "Epoch 14/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6909 - acc: 0.6667\n",
            "Epoch 15/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6907 - acc: 0.6667\n",
            "Epoch 16/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6905 - acc: 0.6667\n",
            "Epoch 17/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6904 - acc: 0.6667\n",
            "Epoch 18/100\n",
            "3/3 [==============================] - 0s 896us/step - loss: 0.6902 - acc: 0.6667\n",
            "Epoch 19/100\n",
            "3/3 [==============================] - 0s 843us/step - loss: 0.6900 - acc: 0.6667\n",
            "Epoch 20/100\n",
            "3/3 [==============================] - 0s 995us/step - loss: 0.6898 - acc: 0.6667\n",
            "Epoch 21/100\n",
            "3/3 [==============================] - 0s 970us/step - loss: 0.6896 - acc: 0.6667\n",
            "Epoch 22/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6895 - acc: 0.6667\n",
            "Epoch 23/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6893 - acc: 0.6667\n",
            "Epoch 24/100\n",
            "3/3 [==============================] - 0s 695us/step - loss: 0.6891 - acc: 0.6667\n",
            "Epoch 25/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6889 - acc: 0.6667\n",
            "Epoch 26/100\n",
            "3/3 [==============================] - 0s 837us/step - loss: 0.6887 - acc: 0.6667\n",
            "Epoch 27/100\n",
            "3/3 [==============================] - 0s 691us/step - loss: 0.6885 - acc: 0.6667\n",
            "Epoch 28/100\n",
            "3/3 [==============================] - 0s 747us/step - loss: 0.6884 - acc: 0.6667\n",
            "Epoch 29/100\n",
            "3/3 [==============================] - 0s 901us/step - loss: 0.6882 - acc: 0.6667\n",
            "Epoch 30/100\n",
            "3/3 [==============================] - 0s 814us/step - loss: 0.6880 - acc: 0.6667\n",
            "Epoch 31/100\n",
            "3/3 [==============================] - 0s 790us/step - loss: 0.6878 - acc: 0.6667\n",
            "Epoch 32/100\n",
            "3/3 [==============================] - 0s 764us/step - loss: 0.6876 - acc: 0.6667\n",
            "Epoch 33/100\n",
            "3/3 [==============================] - 0s 786us/step - loss: 0.6874 - acc: 0.6667\n",
            "Epoch 34/100\n",
            "3/3 [==============================] - 0s 686us/step - loss: 0.6872 - acc: 0.6667\n",
            "Epoch 35/100\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.6870 - acc: 0.6667\n",
            "Epoch 36/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6868 - acc: 0.6667\n",
            "Epoch 37/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6866 - acc: 0.6667\n",
            "Epoch 38/100\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.6864 - acc: 0.6667\n",
            "Epoch 39/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6861 - acc: 0.6667\n",
            "Epoch 40/100\n",
            "3/3 [==============================] - 0s 873us/step - loss: 0.6859 - acc: 0.6667\n",
            "Epoch 41/100\n",
            "3/3 [==============================] - 0s 831us/step - loss: 0.6857 - acc: 0.6667\n",
            "Epoch 42/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6855 - acc: 0.6667\n",
            "Epoch 43/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6853 - acc: 0.6667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44/100\n",
            "\r3/3 [==============================] - 0s 1ms/step - loss: 0.6850 - acc: 0.6667\n",
            "Epoch 45/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6848 - acc: 0.6667\n",
            "Epoch 46/100\n",
            "3/3 [==============================] - 0s 821us/step - loss: 0.6846 - acc: 0.6667\n",
            "Epoch 47/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6843 - acc: 0.6667\n",
            "Epoch 48/100\n",
            "3/3 [==============================] - 0s 783us/step - loss: 0.6841 - acc: 0.6667\n",
            "Epoch 49/100\n",
            "3/3 [==============================] - 0s 890us/step - loss: 0.6838 - acc: 0.6667\n",
            "Epoch 50/100\n",
            "3/3 [==============================] - 0s 909us/step - loss: 0.6836 - acc: 0.6667\n",
            "Epoch 51/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6833 - acc: 0.6667\n",
            "Epoch 52/100\n",
            "3/3 [==============================] - 0s 869us/step - loss: 0.6831 - acc: 0.6667\n",
            "Epoch 53/100\n",
            "3/3 [==============================] - 0s 745us/step - loss: 0.6828 - acc: 0.6667\n",
            "Epoch 54/100\n",
            "3/3 [==============================] - 0s 959us/step - loss: 0.6825 - acc: 0.6667\n",
            "Epoch 55/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6823 - acc: 0.6667\n",
            "Epoch 56/100\n",
            "3/3 [==============================] - 0s 842us/step - loss: 0.6820 - acc: 0.6667\n",
            "Epoch 57/100\n",
            "3/3 [==============================] - 0s 923us/step - loss: 0.6817 - acc: 0.6667\n",
            "Epoch 58/100\n",
            "3/3 [==============================] - 0s 775us/step - loss: 0.6814 - acc: 0.6667\n",
            "Epoch 59/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6811 - acc: 0.6667\n",
            "Epoch 60/100\n",
            "3/3 [==============================] - 0s 748us/step - loss: 0.6808 - acc: 0.6667\n",
            "Epoch 61/100\n",
            "3/3 [==============================] - 0s 647us/step - loss: 0.6805 - acc: 0.6667\n",
            "Epoch 62/100\n",
            "3/3 [==============================] - 0s 908us/step - loss: 0.6802 - acc: 0.6667\n",
            "Epoch 63/100\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.6799 - acc: 0.6667\n",
            "Epoch 64/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6796 - acc: 0.6667\n",
            "Epoch 65/100\n",
            "3/3 [==============================] - 0s 974us/step - loss: 0.6792 - acc: 0.6667\n",
            "Epoch 66/100\n",
            "3/3 [==============================] - 0s 689us/step - loss: 0.6789 - acc: 0.6667\n",
            "Epoch 67/100\n",
            "3/3 [==============================] - 0s 793us/step - loss: 0.6785 - acc: 0.6667\n",
            "Epoch 68/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6782 - acc: 0.6667\n",
            "Epoch 69/100\n",
            "3/3 [==============================] - 0s 633us/step - loss: 0.6778 - acc: 0.6667\n",
            "Epoch 70/100\n",
            "3/3 [==============================] - 0s 662us/step - loss: 0.6775 - acc: 0.6667\n",
            "Epoch 71/100\n",
            "3/3 [==============================] - 0s 757us/step - loss: 0.6771 - acc: 0.6667\n",
            "Epoch 72/100\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6767 - acc: 0.6667\n",
            "Epoch 73/100\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.6763 - acc: 0.6667\n",
            "Epoch 74/100\n",
            "3/3 [==============================] - 0s 759us/step - loss: 0.6759 - acc: 0.6667\n",
            "Epoch 75/100\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.6755 - acc: 0.6667\n",
            "Epoch 76/100\n",
            "3/3 [==============================] - 0s 745us/step - loss: 0.6751 - acc: 0.6667\n",
            "Epoch 77/100\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.6747 - acc: 0.6667\n",
            "Epoch 78/100\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.6743 - acc: 0.6667\n",
            "Epoch 79/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6738 - acc: 0.6667\n",
            "Epoch 80/100"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "3/3 [==============================] - 0s 953us/step - loss: 0.6734 - acc: 0.6667\n",
            "Epoch 81/100\n",
            "3/3 [==============================] - 0s 814us/step - loss: 0.6729 - acc: 0.6667\n",
            "Epoch 82/100\n",
            "3/3 [==============================] - 0s 813us/step - loss: 0.6724 - acc: 0.6667\n",
            "Epoch 83/100\n",
            "3/3 [==============================] - 0s 526us/step - loss: 0.6720 - acc: 0.6667\n",
            "Epoch 84/100\n",
            "3/3 [==============================] - 0s 695us/step - loss: 0.6715 - acc: 0.6667\n",
            "Epoch 85/100\n",
            "3/3 [==============================] - 0s 637us/step - loss: 0.6710 - acc: 0.6667\n",
            "Epoch 86/100\n",
            "3/3 [==============================] - 0s 897us/step - loss: 0.6705 - acc: 0.6667\n",
            "Epoch 87/100\n",
            "3/3 [==============================] - 0s 907us/step - loss: 0.6700 - acc: 0.6667\n",
            "Epoch 88/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6694 - acc: 0.6667\n",
            "Epoch 89/100\n",
            "3/3 [==============================] - 0s 693us/step - loss: 0.6689 - acc: 0.6667\n",
            "Epoch 90/100\n",
            "3/3 [==============================] - 0s 754us/step - loss: 0.6684 - acc: 0.6667\n",
            "Epoch 91/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6667\n",
            "Epoch 92/100\n",
            "3/3 [==============================] - 0s 660us/step - loss: 0.6672 - acc: 0.6667\n",
            "Epoch 93/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6666 - acc: 0.6667\n",
            "Epoch 94/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6661 - acc: 0.6667\n",
            "Epoch 95/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6655 - acc: 0.6667\n",
            "Epoch 96/100\n",
            "3/3 [==============================] - 0s 861us/step - loss: 0.6648 - acc: 0.6667\n",
            "Epoch 97/100\n",
            "3/3 [==============================] - 0s 856us/step - loss: 0.6642 - acc: 0.6667\n",
            "Epoch 98/100\n",
            "3/3 [==============================] - 0s 858us/step - loss: 0.6636 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "3/3 [==============================] - 0s 1ms/step - loss: 0.6629 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "3/3 [==============================] - 0s 890us/step - loss: 0.6623 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LSkUXb6HZRdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "e66ce322-5d2e-487e-ba9d-f8876ee294ef"
      },
      "cell_type": "code",
      "source": [
        "print(y_pred)\n",
        "print(confusion_matrix(y,y_pred))\n",
        "print(classification_report(y,y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]]\n",
            "[[0 1]\n",
            " [0 2]]\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.00      0.00      0.00         1\n",
            "          1       0.67      1.00      0.80         2\n",
            "\n",
            "avg / total       0.44      0.67      0.53         3\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4zCR2cyFZrfe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}